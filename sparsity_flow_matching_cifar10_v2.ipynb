{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity-Aware Flow Matching Model for CIFAR-10 (v2)\n",
    "\n",
    "This notebook implements a **Flow Matching** approach (instead of diffusion) for sparsity-aware image reconstruction.\n",
    "\n",
    "**Key Innovation:**\n",
    "- Uses **Conditional Flow Matching** with optimal transport paths\n",
    "- Provides 20% of pixels as conditioning\n",
    "- Trains on a different 20% of pixels\n",
    "- Model learns to reconstruct the full image (100%)\n",
    "- Supports both **ODE (deterministic)** and **SDE (stochastic)** sampling\n",
    "\n",
    "**Advantages over Diffusion:**\n",
    "- Faster sampling (10-50 steps vs 1000 steps)\n",
    "- Straighter probability paths (optimal transport)\n",
    "- More stable training\n",
    "- Flexible sampling (deterministic or stochastic)\n",
    "\n",
    "**Dataset:** CIFAR-10 (32x32 RGB images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sparsity Controller (Same as v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsityController:\n",
    "    \"\"\"\n",
    "    Manages sparsity patterns for training.\n",
    "    \n",
    "    Key behaviors:\n",
    "    - random_epoch: Generate same masks for same sample_id within epoch, new masks next epoch\n",
    "    - random: Completely random masks every time\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, mode='random_epoch', pattern='random', \n",
    "                 sparsity=0.2, block_size=4, num_blocks=5):\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        self.pattern = pattern\n",
    "        self.sparsity = sparsity\n",
    "        self.block_size = block_size\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # Store masks per epoch\n",
    "        self.epoch_cache = {}\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def new_epoch(self):\n",
    "        \"\"\"Call this at the start of each epoch to regenerate masks\"\"\"\n",
    "        self.current_epoch += 1\n",
    "        self.epoch_cache = {}\n",
    "        \n",
    "    def _generate_random_mask(self, C, H, W, rng):\n",
    "        \"\"\"Generate random binary mask with given sparsity\"\"\"\n",
    "        total_pixels = H * W\n",
    "        num_sparse = int(total_pixels * self.sparsity)\n",
    "        \n",
    "        mask = torch.zeros(C, H, W)\n",
    "        for c in range(C):\n",
    "            indices = rng.choice(total_pixels, size=num_sparse, replace=False)\n",
    "            flat_mask = torch.zeros(total_pixels)\n",
    "            flat_mask[indices] = 1.0\n",
    "            mask[c] = flat_mask.reshape(H, W)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def _generate_masks_for_sample(self, C, sample_id):\n",
    "        \"\"\"Generate both conditioning and target masks for a sample\"\"\"\n",
    "        H, W = self.image_size, self.image_size\n",
    "        \n",
    "        # Use sample_id + epoch as seed for reproducibility within epoch\n",
    "        if self.mode == 'random_epoch':\n",
    "            seed = hash((sample_id, self.current_epoch)) % (2**32)\n",
    "        else:\n",
    "            seed = np.random.randint(0, 2**32)\n",
    "        \n",
    "        rng = np.random.RandomState(seed)\n",
    "        \n",
    "        if self.pattern == 'random':\n",
    "            # Generate conditioning mask\n",
    "            cond_mask = self._generate_random_mask(C, H, W, rng)\n",
    "            \n",
    "            # Generate target mask (non-overlapping with cond_mask)\n",
    "            available_pixels = (1 - cond_mask).bool()\n",
    "            target_mask = torch.zeros(C, H, W)\n",
    "            \n",
    "            for c in range(C):\n",
    "                available_indices = torch.where(available_pixels[c].flatten())[0].numpy()\n",
    "                num_target = int(H * W * self.sparsity)\n",
    "                if len(available_indices) >= num_target:\n",
    "                    target_indices = rng.choice(available_indices, size=num_target, replace=False)\n",
    "                    flat_mask = torch.zeros(H * W)\n",
    "                    flat_mask[target_indices] = 1.0\n",
    "                    target_mask[c] = flat_mask.reshape(H, W)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Pattern {self.pattern} not implemented\")\n",
    "        \n",
    "        return cond_mask, target_mask\n",
    "    \n",
    "    def get_masks(self, batch_size, num_channels, sample_ids):\n",
    "        \"\"\"\n",
    "        Get masks for a batch of samples.\n",
    "        \n",
    "        Returns:\n",
    "            cond_masks: List of conditioning masks\n",
    "            target_masks: List of target masks\n",
    "        \"\"\"\n",
    "        cond_masks = []\n",
    "        target_masks = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            sample_id = sample_ids[i]\n",
    "            \n",
    "            # Check cache for random_epoch mode\n",
    "            if self.mode == 'random_epoch':\n",
    "                cache_key = (sample_id, self.current_epoch)\n",
    "                if cache_key in self.epoch_cache:\n",
    "                    cond_mask, target_mask = self.epoch_cache[cache_key]\n",
    "                else:\n",
    "                    cond_mask, target_mask = self._generate_masks_for_sample(num_channels, sample_id)\n",
    "                    self.epoch_cache[cache_key] = (cond_mask, target_mask)\n",
    "            else:\n",
    "                cond_mask, target_mask = self._generate_masks_for_sample(num_channels, sample_id)\n",
    "            \n",
    "            cond_masks.append(cond_mask)\n",
    "            target_masks.append(target_mask)\n",
    "        \n",
    "        return cond_masks, target_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for time t ‚àà [0, 1]\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    \"\"\"Split num into groups of size divisor\"\"\"\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def visualize_samples(images, sparse_inputs, masks, predictions, target_masks, nrow=4, title=\"Samples\"):\n",
    "    \"\"\"Visualize original, sparse input, masks, and predictions\"\"\"\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(15, 15))\n",
    "    \n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    def denorm(x):\n",
    "        return (x + 1) / 2\n",
    "    \n",
    "    # Original images\n",
    "    grid = make_grid(denorm(images[:nrow*nrow]), nrow=nrow)\n",
    "    axes[0].imshow(grid.permute(1, 2, 0).cpu())\n",
    "    axes[0].set_title(\"Original Images\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Sparse inputs (conditioning)\n",
    "    grid = make_grid(denorm(sparse_inputs[:nrow*nrow]), nrow=nrow)\n",
    "    axes[1].imshow(grid.permute(1, 2, 0).cpu())\n",
    "    axes[1].set_title(\"Sparse Conditioning (20%)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Conditioning masks\n",
    "    grid = make_grid(masks[:nrow*nrow], nrow=nrow)\n",
    "    axes[2].imshow(grid.permute(1, 2, 0).cpu(), cmap='gray')\n",
    "    axes[2].set_title(\"Conditioning Mask\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Target masks\n",
    "    grid = make_grid(target_masks[:nrow*nrow], nrow=nrow)\n",
    "    axes[3].imshow(grid.permute(1, 2, 0).cpu(), cmap='gray')\n",
    "    axes[3].set_title(\"Target Loss Mask (different 20%)\")\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    grid = make_grid(denorm(predictions[:nrow*nrow]), nrow=nrow)\n",
    "    axes[4].imshow(grid.permute(1, 2, 0).cpu())\n",
    "    axes[4].set_title(\"Reconstructed Images\")\n",
    "    axes[4].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. U-Net Architecture Components (Same as v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=0.0, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim, self.dim_out = dim, dim_out\n",
    "        dim_out = dim if dim_out is None else dim_out\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(dim, dim_out, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(time_emb_dim, dim_out)\n",
    "        ) if time_emb_dim is not None else None\n",
    "        \n",
    "        self.norm2 = nn.GroupNorm(num_groups=groups, num_channels=dim_out)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.residual_conv = nn.Conv2d(dim, dim_out, kernel_size=1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.conv1(self.activation1(self.norm1(x)))\n",
    "        \n",
    "        if time_emb is not None and self.mlp is not None:\n",
    "            h = h + self.mlp(time_emb)[..., None, None]\n",
    "        \n",
    "        h = self.conv2(self.dropout(self.activation2(self.norm2(h))))\n",
    "        return h + self.residual_conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = dim ** (-0.5)\n",
    "        \n",
    "        self.norm = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.to_qkv = nn.Conv2d(dim, dim * 3, kernel_size=1)\n",
    "        self.to_out = nn.Conv2d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(self.norm(x)).chunk(3, dim=1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b c h w -> b (h w) c'), qkv)\n",
    "        \n",
    "        similarity = torch.einsum('b i c, b j c -> b i j', q, k) * self.scale\n",
    "        attention_score = torch.softmax(similarity, dim=-1)\n",
    "        attention = torch.einsum('b i j, b j c -> b i c', attention_score, v)\n",
    "        \n",
    "        out = rearrange(attention, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        return self.to_out(out) + x\n",
    "\n",
    "\n",
    "class ResnetAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=0.0, groups=32):\n",
    "        super().__init__()\n",
    "        self.resnet = ResnetBlock(dim, dim_out, time_emb_dim, dropout, groups)\n",
    "        self.attention = Attention(dim_out if dim_out else dim, groups)\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        x = self.resnet(x, time_emb)\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Conv2d(dim_in, dim_in, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Velocity Network (U-Net for Flow Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelocityNet(nn.Module):\n",
    "    \"\"\"U-Net that predicts velocity field v_t for flow matching\"\"\"\n",
    "    def __init__(self, dim=64, image_size=32, dim_multiply=(1, 2, 4, 8), \n",
    "                 channel=3, num_res_blocks=2, attn_resolutions=(16,), \n",
    "                 dropout=0.0, groups=32):\n",
    "        super().__init__()\n",
    "        assert dim % groups == 0\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.channel = channel\n",
    "        self.time_emb_dim = 4 * dim\n",
    "        self.num_resolutions = len(dim_multiply)\n",
    "        self.resolution = [int(image_size / (2 ** i)) for i in range(self.num_resolutions)]\n",
    "        self.hidden_dims = [dim, *map(lambda x: x * dim, dim_multiply)]\n",
    "        \n",
    "        # Time embedding (t ‚àà [0, 1])\n",
    "        positional_encoding = PositionalEncoding(dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            positional_encoding,\n",
    "            nn.Linear(dim, self.time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.time_emb_dim, self.time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Initial convolution (3x channels for concatenated input)\n",
    "        self.init_conv = nn.Conv2d(channel * 3, dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Downward path\n",
    "        self.down_path = nn.ModuleList([])\n",
    "        concat_dims = [dim]\n",
    "        \n",
    "        for level in range(self.num_resolutions):\n",
    "            d_in = self.hidden_dims[level]\n",
    "            d_out = self.hidden_dims[level + 1]\n",
    "            \n",
    "            for block in range(num_res_blocks):\n",
    "                d_in_ = d_in if block == 0 else d_out\n",
    "                \n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.down_path.append(\n",
    "                        ResnetAttentionBlock(d_in_, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "                else:\n",
    "                    self.down_path.append(\n",
    "                        ResnetBlock(d_in_, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "                concat_dims.append(d_out)\n",
    "            \n",
    "            if level != self.num_resolutions - 1:\n",
    "                self.down_path.append(DownSample(d_out))\n",
    "                concat_dims.append(d_out)\n",
    "        \n",
    "        # Middle\n",
    "        mid_dim = self.hidden_dims[-1]\n",
    "        self.middle_resnet_attention = ResnetAttentionBlock(\n",
    "            mid_dim, mid_dim, self.time_emb_dim, dropout, groups\n",
    "        )\n",
    "        self.middle_resnet = ResnetBlock(mid_dim, mid_dim, self.time_emb_dim, dropout, groups)\n",
    "        \n",
    "        # Upward path\n",
    "        self.up_path = nn.ModuleList([])\n",
    "        \n",
    "        for level in reversed(range(self.num_resolutions)):\n",
    "            d_out = self.hidden_dims[level + 1]\n",
    "            \n",
    "            for block in range(num_res_blocks + 1):\n",
    "                d_in = self.hidden_dims[level + 2] if block == 0 and level != self.num_resolutions - 1 else d_out\n",
    "                d_in = d_in + concat_dims.pop()\n",
    "                \n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.up_path.append(\n",
    "                        ResnetAttentionBlock(d_in, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "                else:\n",
    "                    self.up_path.append(\n",
    "                        ResnetBlock(d_in, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "            \n",
    "            if level != 0:\n",
    "                self.up_path.append(UpSample(d_out))\n",
    "        \n",
    "        # Output (predict velocity)\n",
    "        final_ch = self.hidden_dims[1]\n",
    "        self.final_norm = nn.GroupNorm(groups, final_ch)\n",
    "        self.final_activation = nn.SiLU()\n",
    "        self.final_conv = nn.Conv2d(final_ch, channel, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        \"\"\"Forward pass: predict velocity field v_t\"\"\"\n",
    "        t = self.time_mlp(time)\n",
    "        \n",
    "        # Downward\n",
    "        concat = []\n",
    "        x = self.init_conv(x)\n",
    "        concat.append(x)\n",
    "        \n",
    "        for layer in self.down_path:\n",
    "            if isinstance(layer, (UpSample, DownSample)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x, t)\n",
    "            concat.append(x)\n",
    "        \n",
    "        # Middle\n",
    "        x = self.middle_resnet_attention(x, t)\n",
    "        x = self.middle_resnet(x, t)\n",
    "        \n",
    "        # Upward\n",
    "        for layer in self.up_path:\n",
    "            if not isinstance(layer, UpSample):\n",
    "                x = torch.cat((x, concat.pop()), dim=1)\n",
    "                x = layer(x, t)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        # Final\n",
    "        x = self.final_activation(self.final_norm(x))\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Flow Matching Model\n",
    "\n",
    "Flow matching learns a velocity field that transports samples from noise to data.\n",
    "\n",
    "### Theory:\n",
    "- **Forward path**: $x_t = t \\cdot x_1 + (1-t) \\cdot x_0$, where $t \\in [0,1]$\n",
    "- **Velocity**: $v_t = x_1 - x_0$ (target velocity)\n",
    "- **Training loss**: $\\mathcal{L} = \\mathbb{E}_{t, x_0, x_1}[\\|v_\\theta(x_t, t) - v_t\\|^2]$\n",
    "- **Sampling**: Solve ODE $\\frac{dx}{dt} = v_\\theta(x_t, t)$ from $t=0$ to $t=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalFlowMatching(nn.Module):\n",
    "    \"\"\"Flow Matching with sparse conditioning\"\"\"\n",
    "    def __init__(self, velocity_net, image_size, loss_type='l2'):\n",
    "        super().__init__()\n",
    "        self.velocity_net = velocity_net\n",
    "        self.channel = velocity_net.channel\n",
    "        self.device = next(velocity_net.parameters()).device\n",
    "        self.image_size = image_size\n",
    "        self.loss_type = loss_type\n",
    "    \n",
    "    def forward(self, x0, sparse_input=None, mask=None, loss_mask=None):\n",
    "        \"\"\"\n",
    "        Training forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x0: Data samples (B, C, H, W)\n",
    "            sparse_input: Masked input for conditioning (B, C, H, W)\n",
    "            mask: Binary mask showing conditioning pixels (B, C, H, W)\n",
    "            loss_mask: Binary mask for target pixels (B, C, H, W)\n",
    "        \"\"\"\n",
    "        b, c, h, w = x0.shape\n",
    "        assert h == self.image_size and w == self.image_size\n",
    "        \n",
    "        # Sample time uniformly t ~ U[0, 1]\n",
    "        t = torch.rand(b, device=x0.device)\n",
    "        \n",
    "        # Sample noise x1 ~ N(0, I)\n",
    "        x1 = torch.randn_like(x0)\n",
    "        \n",
    "        # Interpolate: x_t = t * x_1 + (1-t) * x_0\n",
    "        t_expanded = t[:, None, None, None]\n",
    "        x_t = t_expanded * x1 + (1 - t_expanded) * x0\n",
    "        \n",
    "        # Target velocity: v_t = x_1 - x_0\n",
    "        v_t = x1 - x0\n",
    "        \n",
    "        # Prepare model input\n",
    "        if sparse_input is not None and mask is not None:\n",
    "            model_input = torch.cat([x_t, sparse_input, mask], dim=1)\n",
    "        else:\n",
    "            model_input = x_t\n",
    "        \n",
    "        # Predict velocity\n",
    "        v_pred = self.velocity_net(model_input, t)\n",
    "        \n",
    "        # Compute loss\n",
    "        if self.loss_type == 'l1':\n",
    "            raw_loss = F.l1_loss(v_pred, v_t, reduction='none')\n",
    "        elif self.loss_type == 'l2':\n",
    "            raw_loss = F.mse_loss(v_pred, v_t, reduction='none')\n",
    "        elif self.loss_type == 'huber':\n",
    "            raw_loss = F.smooth_l1_loss(v_pred, v_t, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        # Apply loss mask (focus on target pixels)\n",
    "        if loss_mask is not None:\n",
    "            # Weight target pixels highly, conditioning pixels slightly\n",
    "            lambda_cond = 0.05\n",
    "            combined_mask = loss_mask + lambda_cond * mask\n",
    "            combined_mask = combined_mask.clamp(max=1.0)\n",
    "            loss = (raw_loss * combined_mask).sum() / combined_mask.sum()\n",
    "        else:\n",
    "            loss = raw_loss.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def sample_ode(self, batch_size, num_steps=50, sparse_input=None, mask=None, method='euler'):\n",
    "        \"\"\"\n",
    "        ODE sampling (deterministic).\n",
    "        \n",
    "        Solves: dx/dt = v_Œ∏(x_t, t) from t=0 to t=1\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples\n",
    "            num_steps: Number of integration steps\n",
    "            method: 'euler' or 'rk4'\n",
    "        \"\"\"\n",
    "        assert sparse_input is not None and mask is not None\n",
    "        \n",
    "        # Start from noise x_0 ~ N(0, I)\n",
    "        x = torch.randn([batch_size, self.channel, self.image_size, self.image_size], \n",
    "                       device=self.device)\n",
    "        \n",
    "        dt = 1.0 / num_steps\n",
    "        \n",
    "        for i in tqdm(range(num_steps), desc='ODE Sampling', leave=False):\n",
    "            t = i / num_steps\n",
    "            t_batch = torch.full((batch_size,), t, device=self.device)\n",
    "            \n",
    "            # Prepare input\n",
    "            model_input = torch.cat([x, sparse_input, mask], dim=1)\n",
    "            \n",
    "            if method == 'euler':\n",
    "                # Euler method: x_{t+dt} = x_t + dt * v_Œ∏(x_t, t)\n",
    "                v = self.velocity_net(model_input, t_batch)\n",
    "                x = x + dt * v\n",
    "            \n",
    "            elif method == 'rk4':\n",
    "                # RK4 (more accurate)\n",
    "                k1 = self.velocity_net(model_input, t_batch)\n",
    "                \n",
    "                x2 = x + 0.5 * dt * k1\n",
    "                t2 = torch.full((batch_size,), t + 0.5 * dt, device=self.device)\n",
    "                k2 = self.velocity_net(torch.cat([x2, sparse_input, mask], dim=1), t2)\n",
    "                \n",
    "                x3 = x + 0.5 * dt * k2\n",
    "                k3 = self.velocity_net(torch.cat([x3, sparse_input, mask], dim=1), t2)\n",
    "                \n",
    "                x4 = x + dt * k3\n",
    "                t4 = torch.full((batch_size,), t + dt, device=self.device)\n",
    "                k4 = self.velocity_net(torch.cat([x4, sparse_input, mask], dim=1), t4)\n",
    "                \n",
    "                x = x + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "        \n",
    "        x.clamp_(-1., 1.)\n",
    "        return x\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def sample_sde(self, batch_size, num_steps=50, sparse_input=None, mask=None, \n",
    "                   noise_scale=0.1):\n",
    "        \"\"\"\n",
    "        SDE sampling (stochastic).\n",
    "        \n",
    "        Solves: dx = v_Œ∏(x_t, t)dt + œÉ(t)dW\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples\n",
    "            num_steps: Number of integration steps\n",
    "            noise_scale: Scaling factor for stochastic noise œÉ(t)\n",
    "        \"\"\"\n",
    "        assert sparse_input is not None and mask is not None\n",
    "        \n",
    "        # Start from noise x_0 ~ N(0, I)\n",
    "        x = torch.randn([batch_size, self.channel, self.image_size, self.image_size], \n",
    "                       device=self.device)\n",
    "        \n",
    "        dt = 1.0 / num_steps\n",
    "        \n",
    "        for i in tqdm(range(num_steps), desc='SDE Sampling', leave=False):\n",
    "            t = i / num_steps\n",
    "            t_batch = torch.full((batch_size,), t, device=self.device)\n",
    "            \n",
    "            # Prepare input\n",
    "            model_input = torch.cat([x, sparse_input, mask], dim=1)\n",
    "            \n",
    "            # Predict velocity\n",
    "            v = self.velocity_net(model_input, t_batch)\n",
    "            \n",
    "            # Time-dependent noise schedule: more noise early, less later\n",
    "            sigma_t = noise_scale * (1 - t)\n",
    "            \n",
    "            # SDE update: dx = v*dt + œÉ*‚àödt*dW\n",
    "            noise = torch.randn_like(x)\n",
    "            x = x + dt * v + sigma_t * math.sqrt(dt) * noise\n",
    "        \n",
    "        x.clamp_(-1., 1.)\n",
    "        return x\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def sample(self, batch_size=16, num_steps=50, sparse_input=None, mask=None, \n",
    "              method='ode', **kwargs):\n",
    "        \"\"\"Unified sampling interface\"\"\"\n",
    "        if method == 'ode':\n",
    "            return self.sample_ode(batch_size, num_steps, sparse_input, mask, \n",
    "                                  kwargs.get('ode_method', 'euler'))\n",
    "        elif method == 'sde':\n",
    "            return self.sample_sde(batch_size, num_steps, sparse_input, mask, \n",
    "                                  kwargs.get('noise_scale', 0.1))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CIFAR-10 Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 2e-4\n",
    "EPOCHS = 50\n",
    "SAVE_EVERY = 5\n",
    "SAMPLE_STEPS = 50  # Flow matching uses fewer steps than diffusion!\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('./results_v2', exist_ok=True)\n",
    "os.makedirs('./checkpoints_v2', exist_ok=True)\n",
    "\n",
    "# Data transforms (normalize to [-1, 1])\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Velocity Network\n",
    "velocity_net = VelocityNet(\n",
    "    dim=64,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    dim_multiply=(1, 2, 4, 8),\n",
    "    channel=3,\n",
    "    num_res_blocks=2,\n",
    "    attn_resolutions=(16,),\n",
    "    dropout=0.1,\n",
    "    groups=32\n",
    ").to(device)\n",
    "\n",
    "# Initialize Flow Matching Model\n",
    "flow_matching = ConditionalFlowMatching(\n",
    "    velocity_net=velocity_net,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    loss_type='l2'\n",
    ").to(device)\n",
    "\n",
    "# Initialize Sparsity Controller\n",
    "sparsity_controller = SparsityController(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    mode='random_epoch',\n",
    "    pattern='random',\n",
    "    sparsity=0.2,\n",
    "    block_size=4,\n",
    "    num_blocks=5\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(flow_matching.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in flow_matching.parameters())\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"Sampling steps: {SAMPLE_STEPS} (vs 1000 for diffusion!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    flow_matching.train()\n",
    "    sparsity_controller.new_epoch()  # Generate new masks for this epoch\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        B, C, H, W = images.shape\n",
    "        \n",
    "        # Generate sample IDs\n",
    "        sample_ids = labels.tolist()\n",
    "        \n",
    "        # Get masks\n",
    "        cond_masks, target_masks = sparsity_controller.get_masks(B, C, sample_ids)\n",
    "        cond_mask = torch.stack(cond_masks).to(device)\n",
    "        target_mask = torch.stack(target_masks).to(device)\n",
    "        \n",
    "        # Create sparse input\n",
    "        sparse_input = images * cond_mask\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss = flow_matching(\n",
    "            images,\n",
    "            sparse_input=sparse_input,\n",
    "            mask=cond_mask,\n",
    "            loss_mask=target_mask\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_matching.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_images(epoch, num_samples=16, method='ode'):\n",
    "    \"\"\"Generate samples using ODE or SDE\"\"\"\n",
    "    flow_matching.eval()\n",
    "    \n",
    "    # Get a batch from test set\n",
    "    test_images, test_labels = next(iter(test_loader))\n",
    "    test_images = test_images[:num_samples].to(device)\n",
    "    \n",
    "    # Generate masks\n",
    "    sample_ids = test_labels[:num_samples].tolist()\n",
    "    cond_masks, target_masks = sparsity_controller.get_masks(num_samples, 3, sample_ids)\n",
    "    cond_mask = torch.stack(cond_masks).to(device)\n",
    "    target_mask = torch.stack(target_masks).to(device)\n",
    "    \n",
    "    # Create sparse input\n",
    "    sparse_input = test_images * cond_mask\n",
    "    \n",
    "    # Sample using specified method\n",
    "    samples = flow_matching.sample(\n",
    "        batch_size=num_samples,\n",
    "        num_steps=SAMPLE_STEPS,\n",
    "        sparse_input=sparse_input,\n",
    "        mask=cond_mask,\n",
    "        method=method,\n",
    "        ode_method='euler',  # or 'rk4' for higher accuracy\n",
    "        noise_scale=0.1  # for SDE\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    fig = visualize_samples(\n",
    "        test_images, sparse_input, cond_mask, samples, target_mask,\n",
    "        nrow=4, title=f\"Epoch {epoch+1} ({method.upper()})\"\n",
    "    )\n",
    "    plt.savefig(f'./results_v2/samples_epoch_{epoch+1:03d}_{method}.png', \n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved {method.upper()} samples for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "train_losses = []\n",
    "\n",
    "print(\"Starting Flow Matching training...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    avg_loss = train_epoch(epoch)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Sample images\n",
    "    if (epoch + 1) % SAVE_EVERY == 0 or epoch == 0:\n",
    "        # Generate both ODE and SDE samples for comparison\n",
    "        sample_images(epoch, num_samples=16, method='ode')\n",
    "        sample_images(epoch, num_samples=16, method='sde')\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': flow_matching.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'train_losses': train_losses\n",
    "        }\n",
    "        torch.save(checkpoint, f'./checkpoints_v2/checkpoint_epoch_{epoch+1:03d}.pt')\n",
    "        print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Flow Matching Training Loss over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('./results_v2/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compare ODE vs SDE Sampling\n",
    "\n",
    "Generate samples with both deterministic (ODE) and stochastic (SDE) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_matching.eval()\n",
    "\n",
    "# Get test samples\n",
    "num_samples = 64\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images = test_images[:num_samples].to(device)\n",
    "\n",
    "sample_ids = test_labels[:num_samples].tolist()\n",
    "cond_masks, target_masks = sparsity_controller.get_masks(num_samples, 3, sample_ids)\n",
    "cond_mask = torch.stack(cond_masks).to(device)\n",
    "target_mask = torch.stack(target_masks).to(device)\n",
    "sparse_input = test_images * cond_mask\n",
    "\n",
    "print(\"Generating ODE samples (deterministic)...\")\n",
    "samples_ode = flow_matching.sample(\n",
    "    batch_size=num_samples,\n",
    "    num_steps=SAMPLE_STEPS,\n",
    "    sparse_input=sparse_input,\n",
    "    mask=cond_mask,\n",
    "    method='ode',\n",
    "    ode_method='euler'\n",
    ")\n",
    "\n",
    "print(\"Generating SDE samples (stochastic)...\")\n",
    "samples_sde = flow_matching.sample(\n",
    "    batch_size=num_samples,\n",
    "    num_steps=SAMPLE_STEPS,\n",
    "    sparse_input=sparse_input,\n",
    "    mask=cond_mask,\n",
    "    method='sde',\n",
    "    noise_scale=0.1\n",
    ")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 8))\n",
    "\n",
    "def denorm(x):\n",
    "    return (x + 1) / 2\n",
    "\n",
    "# ODE samples\n",
    "grid_ode = make_grid(denorm(samples_ode), nrow=8)\n",
    "axes[0].imshow(grid_ode.permute(1, 2, 0).cpu())\n",
    "axes[0].set_title(\"ODE Sampling (Deterministic)\", fontsize=16)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# SDE samples\n",
    "grid_sde = make_grid(denorm(samples_sde), nrow=8)\n",
    "axes[1].imshow(grid_sde.permute(1, 2, 0).cpu())\n",
    "axes[1].set_title(\"SDE Sampling (Stochastic)\", fontsize=16)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_v2/ode_vs_sde_comparison.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Flow Matching training and sampling completed!\")\n",
    "print(f\"Results saved in ./results_v2/\")\n",
    "print(f\"Checkpoints saved in ./checkpoints_v2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Benchmark: Different Step Counts\n",
    "\n",
    "Test how sampling quality varies with number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "step_counts = [10, 20, 50, 100]\n",
    "num_test_samples = 16\n",
    "\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images = test_images[:num_test_samples].to(device)\n",
    "sample_ids = test_labels[:num_test_samples].tolist()\n",
    "cond_masks, target_masks = sparsity_controller.get_masks(num_test_samples, 3, sample_ids)\n",
    "cond_mask = torch.stack(cond_masks).to(device)\n",
    "sparse_input = test_images * cond_mask\n",
    "\n",
    "fig, axes = plt.subplots(len(step_counts), 1, figsize=(15, 4*len(step_counts)))\n",
    "\n",
    "for idx, steps in enumerate(step_counts):\n",
    "    print(f\"\\nSampling with {steps} steps...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    samples = flow_matching.sample(\n",
    "        batch_size=num_test_samples,\n",
    "        num_steps=steps,\n",
    "        sparse_input=sparse_input,\n",
    "        mask=cond_mask,\n",
    "        method='ode'\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Time: {elapsed:.2f}s ({elapsed/num_test_samples:.3f}s per sample)\")\n",
    "    \n",
    "    grid = make_grid(denorm(samples), nrow=4)\n",
    "    axes[idx].imshow(grid.permute(1, 2, 0).cpu())\n",
    "    axes[idx].set_title(f\"{steps} steps ({elapsed:.2f}s)\", fontsize=14)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_v2/step_count_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Flow matching is much faster than diffusion!\")\n",
    "print(\"   - Diffusion: 1000 steps\")\n",
    "print(\"   - Flow Matching: 10-100 steps for similar quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates **Flow Matching** for sparsity-aware image reconstruction:\n",
    "\n",
    "### Key Advantages over Diffusion (v1):\n",
    "\n",
    "1. **‚ö° Faster Sampling**: 10-100 steps vs 1000 steps\n",
    "2. **üìê Straighter Paths**: Optimal transport interpolation\n",
    "3. **üéØ Simpler Training**: Direct velocity prediction\n",
    "4. **üîÄ Flexible Sampling**: Both ODE (deterministic) and SDE (stochastic)\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- ‚úÖ **Conditional Flow Matching** with sparse observations\n",
    "- ‚úÖ **ODE Solver**: Deterministic sampling (Euler or RK4)\n",
    "- ‚úÖ **SDE Solver**: Stochastic sampling with configurable noise\n",
    "- ‚úÖ **Weighted Loss**: Focus on target pixels (1.0) vs conditioning (0.05)\n",
    "- ‚úÖ **Same Architecture**: Can compare directly with diffusion baseline\n",
    "\n",
    "### Sampling Methods:\n",
    "\n",
    "**ODE (Deterministic)**:\n",
    "- Solves: $dx/dt = v_\\theta(x_t, t)$\n",
    "- Same input ‚Üí same output\n",
    "- Best for reproducibility\n",
    "\n",
    "**SDE (Stochastic)**:\n",
    "- Solves: $dx = v_\\theta(x_t, t)dt + \\sigma(t)dW$\n",
    "- Adds controlled randomness\n",
    "- Can produce diverse samples\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Compare with v1**: Flow matching vs diffusion performance\n",
    "2. **Tune sampling**: Try different step counts (10-100)\n",
    "3. **Experiment with SDE noise**: Adjust `noise_scale`\n",
    "4. **Try RK4**: More accurate ODE integration\n",
    "5. **Optimal Transport**: Add minibatch OT for better paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
