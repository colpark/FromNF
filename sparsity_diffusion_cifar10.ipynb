{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity-Aware Diffusion Model for CIFAR-10\n",
    "\n",
    "This notebook implements a sparsity-aware diffusion model that learns to reconstruct full images from sparse observations.\n",
    "\n",
    "**Key Innovation:**\n",
    "- Provides 20% of pixels as conditioning\n",
    "- Trains on a different 20% of pixels\n",
    "- Model learns to reconstruct the full image (100%)\n",
    "\n",
    "**Dataset:** CIFAR-10 (32x32 RGB images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sparsity Controller\n",
    "\n",
    "Controls the generation of sparse masks for conditioning and target loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsityController:\n",
    "    \"\"\"\n",
    "    Manages sparsity patterns for training.\n",
    "    \n",
    "    Key behaviors:\n",
    "    - random_epoch: Generate same masks for same sample_id within epoch, new masks next epoch\n",
    "    - random: Completely random masks every time\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, mode='random_epoch', pattern='random', \n",
    "                 sparsity=0.2, block_size=4, num_blocks=5):\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        self.pattern = pattern\n",
    "        self.sparsity = sparsity\n",
    "        self.block_size = block_size\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # Store masks per epoch\n",
    "        self.epoch_cache = {}\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def new_epoch(self):\n",
    "        \"\"\"Call this at the start of each epoch to regenerate masks\"\"\"\n",
    "        self.current_epoch += 1\n",
    "        self.epoch_cache = {}\n",
    "        \n",
    "    def _generate_random_mask(self, C, H, W, rng):\n",
    "        \"\"\"Generate random binary mask with given sparsity\"\"\"\n",
    "        total_pixels = H * W\n",
    "        num_sparse = int(total_pixels * self.sparsity)\n",
    "        \n",
    "        mask = torch.zeros(C, H, W)\n",
    "        for c in range(C):\n",
    "            indices = rng.choice(total_pixels, size=num_sparse, replace=False)\n",
    "            flat_mask = torch.zeros(total_pixels)\n",
    "            flat_mask[indices] = 1.0\n",
    "            mask[c] = flat_mask.reshape(H, W)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def _generate_masks_for_sample(self, C, sample_id):\n",
    "        \"\"\"Generate both conditioning and target masks for a sample\"\"\"\n",
    "        H, W = self.image_size, self.image_size\n",
    "        \n",
    "        # Use sample_id + epoch as seed for reproducibility within epoch\n",
    "        if self.mode == 'random_epoch':\n",
    "            seed = hash((sample_id, self.current_epoch)) % (2**32)\n",
    "        else:\n",
    "            seed = np.random.randint(0, 2**32)\n",
    "        \n",
    "        rng = np.random.RandomState(seed)\n",
    "        \n",
    "        if self.pattern == 'random':\n",
    "            # Generate conditioning mask\n",
    "            cond_mask = self._generate_random_mask(C, H, W, rng)\n",
    "            \n",
    "            # Generate target mask (non-overlapping with cond_mask)\n",
    "            available_pixels = (1 - cond_mask).bool()\n",
    "            target_mask = torch.zeros(C, H, W)\n",
    "            \n",
    "            for c in range(C):\n",
    "                available_indices = torch.where(available_pixels[c].flatten())[0].numpy()\n",
    "                num_target = int(H * W * self.sparsity)\n",
    "                if len(available_indices) >= num_target:\n",
    "                    target_indices = rng.choice(available_indices, size=num_target, replace=False)\n",
    "                    flat_mask = torch.zeros(H * W)\n",
    "                    flat_mask[target_indices] = 1.0\n",
    "                    target_mask[c] = flat_mask.reshape(H, W)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Pattern {self.pattern} not implemented\")\n",
    "        \n",
    "        return cond_mask, target_mask\n",
    "    \n",
    "    def get_masks(self, batch_size, num_channels, sample_ids):\n",
    "        \"\"\"\n",
    "        Get masks for a batch of samples.\n",
    "        \n",
    "        Returns:\n",
    "            cond_masks: List of conditioning masks\n",
    "            target_masks: List of target masks\n",
    "        \"\"\"\n",
    "        cond_masks = []\n",
    "        target_masks = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            sample_id = sample_ids[i]\n",
    "            \n",
    "            # Check cache for random_epoch mode\n",
    "            if self.mode == 'random_epoch':\n",
    "                cache_key = (sample_id, self.current_epoch)\n",
    "                if cache_key in self.epoch_cache:\n",
    "                    cond_mask, target_mask = self.epoch_cache[cache_key]\n",
    "                else:\n",
    "                    cond_mask, target_mask = self._generate_masks_for_sample(num_channels, sample_id)\n",
    "                    self.epoch_cache[cache_key] = (cond_mask, target_mask)\n",
    "            else:\n",
    "                cond_mask, target_mask = self._generate_masks_for_sample(num_channels, sample_id)\n",
    "            \n",
    "            cond_masks.append(cond_mask)\n",
    "            target_masks.append(target_mask)\n",
    "        \n",
    "        return cond_masks, target_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for timesteps\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    \"\"\"Split num into groups of size divisor\"\"\"\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def visualize_samples(images, sparse_inputs, masks, predictions, target_masks, nrow=4, title=\"Samples\"):\n",
    "    \"\"\"Visualize original, sparse input, masks, and predictions\"\"\"\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(15, 15))\n",
    "    \n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    def denorm(x):\n",
    "        return (x + 1) / 2\n",
    "    \n",
    "    # Original images\n",
    "    grid = make_grid(denorm(images[:nrow*nrow]), nrow=nrow)\n",
    "    axes[0].imshow(grid.permute(1, 2, 0).cpu())\n",
    "    axes[0].set_title(\"Original Images\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Sparse inputs (conditioning)\n",
    "    grid = make_grid(denorm(sparse_inputs[:nrow*nrow]), nrow=nrow)\n",
    "    axes[1].imshow(grid.permute(1, 2, 0).cpu())\n",
    "    axes[1].set_title(\"Sparse Conditioning (20%)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Conditioning masks\n",
    "    grid = make_grid(masks[:nrow*nrow], nrow=nrow)\n",
    "    axes[2].imshow(grid.permute(1, 2, 0).cpu(), cmap='gray')\n",
    "    axes[2].set_title(\"Conditioning Mask\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Target masks\n",
    "    grid = make_grid(target_masks[:nrow*nrow], nrow=nrow)\n",
    "    axes[3].imshow(grid.permute(1, 2, 0).cpu(), cmap='gray')\n",
    "    axes[3].set_title(\"Target Loss Mask (different 20%)\")\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    grid = make_grid(denorm(predictions[:nrow*nrow]), nrow=nrow)\n",
    "    axes[4].imshow(grid.permute(1, 2, 0).cpu())\n",
    "    axes[4].set_title(\"Reconstructed Images\")\n",
    "    axes[4].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. U-Net Architecture Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=0.0, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim, self.dim_out = dim, dim_out\n",
    "        dim_out = dim if dim_out is None else dim_out\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(dim, dim_out, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(time_emb_dim, dim_out)\n",
    "        ) if time_emb_dim is not None else None\n",
    "        \n",
    "        self.norm2 = nn.GroupNorm(num_groups=groups, num_channels=dim_out)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.residual_conv = nn.Conv2d(dim, dim_out, kernel_size=1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.conv1(self.activation1(self.norm1(x)))\n",
    "        \n",
    "        if time_emb is not None and self.mlp is not None:\n",
    "            h = h + self.mlp(time_emb)[..., None, None]\n",
    "        \n",
    "        h = self.conv2(self.dropout(self.activation2(self.norm2(h))))\n",
    "        return h + self.residual_conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = dim ** (-0.5)\n",
    "        \n",
    "        self.norm = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.to_qkv = nn.Conv2d(dim, dim * 3, kernel_size=1)\n",
    "        self.to_out = nn.Conv2d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(self.norm(x)).chunk(3, dim=1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b c h w -> b (h w) c'), qkv)\n",
    "        \n",
    "        similarity = torch.einsum('b i c, b j c -> b i j', q, k) * self.scale\n",
    "        attention_score = torch.softmax(similarity, dim=-1)\n",
    "        attention = torch.einsum('b i j, b j c -> b i c', attention_score, v)\n",
    "        \n",
    "        out = rearrange(attention, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        return self.to_out(out) + x\n",
    "\n",
    "\n",
    "class ResnetAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=0.0, groups=32):\n",
    "        super().__init__()\n",
    "        self.resnet = ResnetBlock(dim, dim_out, time_emb_dim, dropout, groups)\n",
    "        self.attention = Attention(dim_out if dim_out else dim, groups)\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        x = self.resnet(x, time_emb)\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Conv2d(dim_in, dim_in, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, dim=64, image_size=32, dim_multiply=(1, 2, 4, 8), \n",
    "                 channel=3, num_res_blocks=2, attn_resolutions=(16,), \n",
    "                 dropout=0.0, groups=32):\n",
    "        \"\"\"\n",
    "        U-Net for noise prediction with sparse conditioning.\n",
    "        \n",
    "        Input channels: channel * 3 (noised_image + sparse_input + mask)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dim % groups == 0\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.channel = channel\n",
    "        self.time_emb_dim = 4 * dim\n",
    "        self.num_resolutions = len(dim_multiply)\n",
    "        self.resolution = [int(image_size / (2 ** i)) for i in range(self.num_resolutions)]\n",
    "        self.hidden_dims = [dim, *map(lambda x: x * dim, dim_multiply)]\n",
    "        \n",
    "        # Time embedding\n",
    "        positional_encoding = PositionalEncoding(dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            positional_encoding,\n",
    "            nn.Linear(dim, self.time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.time_emb_dim, self.time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Initial convolution (3x channels for concatenated input)\n",
    "        self.init_conv = nn.Conv2d(channel * 3, dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Downward path\n",
    "        self.down_path = nn.ModuleList([])\n",
    "        concat_dims = [dim]\n",
    "        \n",
    "        for level in range(self.num_resolutions):\n",
    "            d_in = self.hidden_dims[level]\n",
    "            d_out = self.hidden_dims[level + 1]\n",
    "            \n",
    "            for block in range(num_res_blocks):\n",
    "                d_in_ = d_in if block == 0 else d_out\n",
    "                \n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.down_path.append(\n",
    "                        ResnetAttentionBlock(d_in_, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "                else:\n",
    "                    self.down_path.append(\n",
    "                        ResnetBlock(d_in_, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "                concat_dims.append(d_out)\n",
    "            \n",
    "            if level != self.num_resolutions - 1:\n",
    "                self.down_path.append(DownSample(d_out))\n",
    "                concat_dims.append(d_out)\n",
    "        \n",
    "        # Middle\n",
    "        mid_dim = self.hidden_dims[-1]\n",
    "        self.middle_resnet_attention = ResnetAttentionBlock(\n",
    "            mid_dim, mid_dim, self.time_emb_dim, dropout, groups\n",
    "        )\n",
    "        self.middle_resnet = ResnetBlock(mid_dim, mid_dim, self.time_emb_dim, dropout, groups)\n",
    "        \n",
    "        # Upward path\n",
    "        self.up_path = nn.ModuleList([])\n",
    "        \n",
    "        for level in reversed(range(self.num_resolutions)):\n",
    "            d_out = self.hidden_dims[level + 1]\n",
    "            \n",
    "            for block in range(num_res_blocks + 1):\n",
    "                d_in = self.hidden_dims[level + 2] if block == 0 and level != self.num_resolutions - 1 else d_out\n",
    "                d_in = d_in + concat_dims.pop()\n",
    "                \n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.up_path.append(\n",
    "                        ResnetAttentionBlock(d_in, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "                else:\n",
    "                    self.up_path.append(\n",
    "                        ResnetBlock(d_in, d_out, self.time_emb_dim, dropout, groups)\n",
    "                    )\n",
    "            \n",
    "            if level != 0:\n",
    "                self.up_path.append(UpSample(d_out))\n",
    "        \n",
    "        # Output\n",
    "        final_ch = self.hidden_dims[1]\n",
    "        self.final_norm = nn.GroupNorm(groups, final_ch)\n",
    "        self.final_activation = nn.SiLU()\n",
    "        self.final_conv = nn.Conv2d(final_ch, channel, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        \"\"\"Forward pass with concatenated [noised_image, sparse_input, mask]\"\"\"\n",
    "        t = self.time_mlp(time)\n",
    "        \n",
    "        # Downward\n",
    "        concat = []\n",
    "        x = self.init_conv(x)\n",
    "        concat.append(x)\n",
    "        \n",
    "        for layer in self.down_path:\n",
    "            if isinstance(layer, (UpSample, DownSample)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x, t)\n",
    "            concat.append(x)\n",
    "        \n",
    "        # Middle\n",
    "        x = self.middle_resnet_attention(x, t)\n",
    "        x = self.middle_resnet(x, t)\n",
    "        \n",
    "        # Upward\n",
    "        for layer in self.up_path:\n",
    "            if not isinstance(layer, UpSample):\n",
    "                x = torch.cat((x, concat.pop()), dim=1)\n",
    "                x = layer(x, t)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        # Final\n",
    "        x = self.final_activation(self.final_norm(x))\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gaussian Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, model, image_size, time_step=1000, loss_type='l2'):\n",
    "        \"\"\"\n",
    "        Gaussian Diffusion with sparse conditioning.\n",
    "        \n",
    "        Args:\n",
    "            model: U-Net denoising network\n",
    "            image_size: Image resolution\n",
    "            time_step: Number of diffusion steps (T)\n",
    "            loss_type: 'l1', 'l2', or 'huber'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.unet = model\n",
    "        self.channel = self.unet.channel\n",
    "        self.device = next(self.unet.parameters()).device\n",
    "        self.image_size = image_size\n",
    "        self.time_step = time_step\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        # Noise schedule\n",
    "        beta = self.linear_beta_schedule()\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "        alpha_bar_prev = F.pad(alpha_bar[:-1], pad=(1, 0), value=1.)\n",
    "        \n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "        self.register_buffer('alpha_bar_prev', alpha_bar_prev)\n",
    "        \n",
    "        # For q(x_t | x_0)\n",
    "        self.register_buffer('sqrt_alpha_bar', torch.sqrt(alpha_bar))\n",
    "        self.register_buffer('sqrt_one_minus_alpha_bar', torch.sqrt(1 - alpha_bar))\n",
    "        \n",
    "        # For q(x_{t-1} | x_t, x_0)\n",
    "        self.register_buffer('beta_tilde', beta * ((1. - alpha_bar_prev) / (1. - alpha_bar)))\n",
    "        self.register_buffer('mean_tilde_x0_coeff', beta * torch.sqrt(alpha_bar_prev) / (1 - alpha_bar))\n",
    "        self.register_buffer('mean_tilde_xt_coeff', torch.sqrt(alpha) * (1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "        \n",
    "        # For predicted x0\n",
    "        self.register_buffer('sqrt_recip_alpha_bar', torch.sqrt(1. / alpha_bar))\n",
    "        self.register_buffer('sqrt_recip_alpha_bar_min_1', torch.sqrt(1. / alpha_bar - 1))\n",
    "        \n",
    "        # For sampling\n",
    "        self.register_buffer('sqrt_recip_alpha', torch.sqrt(1. / alpha))\n",
    "        self.register_buffer('beta_over_sqrt_one_minus_alpha_bar', beta / torch.sqrt(1. - alpha_bar))\n",
    "    \n",
    "    def linear_beta_schedule(self):\n",
    "        \"\"\"Linear beta schedule from DDPM paper\"\"\"\n",
    "        scale = 1000 / self.time_step\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, self.time_step, dtype=torch.float32)\n",
    "    \n",
    "    def q_sample(self, x0, t, noise):\n",
    "        \"\"\"Sample x_t from q(x_t | x_0) using reparameterization trick\"\"\"\n",
    "        return (self.sqrt_alpha_bar[t][:, None, None, None] * x0 +\n",
    "                self.sqrt_one_minus_alpha_bar[t][:, None, None, None] * noise)\n",
    "    \n",
    "    def forward(self, img, sparse_input=None, mask=None, loss_mask=None):\n",
    "        \"\"\"\n",
    "        Training forward pass.\n",
    "        \n",
    "        Args:\n",
    "            img: Original images (B, C, H, W)\n",
    "            sparse_input: Masked input for conditioning (B, C, H, W)\n",
    "            mask: Binary mask showing conditioning pixels (B, C, H, W)\n",
    "            loss_mask: Binary mask for target pixels (B, C, H, W)\n",
    "        \"\"\"\n",
    "        b, c, h, w = img.shape\n",
    "        assert h == self.image_size and w == self.image_size\n",
    "        \n",
    "        # Sample random timestep\n",
    "        t = torch.randint(0, self.time_step, (b,), device=img.device).long()\n",
    "        \n",
    "        # Add noise\n",
    "        noise = torch.randn_like(img)\n",
    "        noised_image = self.q_sample(img, t, noise)\n",
    "        \n",
    "        # Prepare model input\n",
    "        if sparse_input is not None and mask is not None:\n",
    "            model_input = torch.cat([noised_image, sparse_input, mask], dim=1)\n",
    "        else:\n",
    "            model_input = noised_image\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = self.unet(model_input, t)\n",
    "        \n",
    "        # Compute loss\n",
    "        if self.loss_type == 'l1':\n",
    "            raw_loss = F.l1_loss(noise, predicted_noise, reduction='none')\n",
    "        elif self.loss_type == 'l2':\n",
    "            raw_loss = F.mse_loss(noise, predicted_noise, reduction='none')\n",
    "        elif self.loss_type == 'huber':\n",
    "            raw_loss = F.smooth_l1_loss(noise, predicted_noise, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        # Apply loss mask (focus on target pixels)\n",
    "        if loss_mask is not None:\n",
    "            # Weight target pixels highly, conditioning pixels slightly\n",
    "            lambda_cond = 0.05\n",
    "            combined_mask = loss_mask + lambda_cond * mask\n",
    "            combined_mask = combined_mask.clamp(max=1.0)\n",
    "            loss = (raw_loss * combined_mask).sum() / combined_mask.sum()\n",
    "        else:\n",
    "            loss = raw_loss.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def p_sample(self, xt, t, clip=True, sparse_input=None, mask=None):\n",
    "        \"\"\"Sample x_{t-1} from p(x_{t-1} | x_t)\"\"\"\n",
    "        batched_time = torch.full((xt.shape[0],), t, device=self.device, dtype=torch.long)\n",
    "        \n",
    "        # Prepare input\n",
    "        if sparse_input is not None and mask is not None:\n",
    "            model_input = torch.cat([xt, sparse_input, mask], dim=1)\n",
    "        else:\n",
    "            model_input = xt\n",
    "        \n",
    "        pred_noise = self.unet(model_input, batched_time)\n",
    "        \n",
    "        # Compute mean\n",
    "        if clip:\n",
    "            x0 = self.sqrt_recip_alpha_bar[t] * xt - self.sqrt_recip_alpha_bar_min_1[t] * pred_noise\n",
    "            x0.clamp_(-1., 1.)\n",
    "            mean = self.mean_tilde_x0_coeff[t] * x0 + self.mean_tilde_xt_coeff[t] * xt\n",
    "        else:\n",
    "            mean = self.sqrt_recip_alpha[t] * (xt - self.beta_over_sqrt_one_minus_alpha_bar[t] * pred_noise)\n",
    "        \n",
    "        variance = self.beta_tilde[t]\n",
    "        noise = torch.randn_like(xt) if t > 0 else 0.\n",
    "        \n",
    "        return mean + torch.sqrt(variance) * noise\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def sample(self, batch_size=16, sparse_input=None, mask=None, clip=True):\n",
    "        \"\"\"Generate samples via reverse diffusion\"\"\"\n",
    "        assert sparse_input is not None and mask is not None, \"Must provide sparse_input and mask\"\n",
    "        \n",
    "        # Start from random noise\n",
    "        xT = torch.randn([batch_size, self.channel, self.image_size, self.image_size], device=self.device)\n",
    "        \n",
    "        xt = xT\n",
    "        for t in tqdm(reversed(range(0, self.time_step)), desc='Sampling', total=self.time_step, leave=False):\n",
    "            xt = self.p_sample(xt, t, clip, sparse_input, mask)\n",
    "        \n",
    "        # Clamp to valid range\n",
    "        xt.clamp_(-1., 1.)\n",
    "        return xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CIFAR-10 Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 2e-4\n",
    "EPOCHS = 50\n",
    "SAVE_EVERY = 5\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "# Data transforms (normalize to [-1, 1])\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize U-Net\n",
    "unet = Unet(\n",
    "    dim=64,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    dim_multiply=(1, 2, 4, 8),\n",
    "    channel=3,\n",
    "    num_res_blocks=2,\n",
    "    attn_resolutions=(16,),\n",
    "    dropout=0.1,\n",
    "    groups=32\n",
    ").to(device)\n",
    "\n",
    "# Initialize Diffusion Model\n",
    "diffusion = GaussianDiffusion(\n",
    "    model=unet,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    time_step=1000,\n",
    "    loss_type='l2'\n",
    ").to(device)\n",
    "\n",
    "# Initialize Sparsity Controller\n",
    "sparsity_controller = SparsityController(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    mode='random_epoch',\n",
    "    pattern='random',\n",
    "    sparsity=0.2,\n",
    "    block_size=4,\n",
    "    num_blocks=5\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(diffusion.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in diffusion.parameters())\n",
    "print(f\"Total parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    diffusion.train()\n",
    "    sparsity_controller.new_epoch()  # Generate new masks for this epoch\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        B, C, H, W = images.shape\n",
    "        \n",
    "        # Generate sample IDs (use label as pseudo-ID for reproducibility)\n",
    "        sample_ids = labels.tolist()\n",
    "        \n",
    "        # Get masks\n",
    "        cond_masks, target_masks = sparsity_controller.get_masks(B, C, sample_ids)\n",
    "        cond_mask = torch.stack(cond_masks).to(device)\n",
    "        target_mask = torch.stack(target_masks).to(device)\n",
    "        \n",
    "        # Create sparse input\n",
    "        sparse_input = images * cond_mask\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion(\n",
    "            images,\n",
    "            sparse_input=sparse_input,\n",
    "            mask=cond_mask,\n",
    "            loss_mask=target_mask\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(diffusion.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Visualize first batch of first epoch\n",
    "        if epoch == 0 and batch_idx == 0:\n",
    "            print(f\"\\nCond mask mean: {cond_mask.mean():.4f}, Target mask mean: {target_mask.mean():.4f}\")\n",
    "    \n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_images(epoch, num_samples=16):\n",
    "    \"\"\"Generate samples and save visualizations\"\"\"\n",
    "    diffusion.eval()\n",
    "    \n",
    "    # Get a batch from test set for conditioning\n",
    "    test_images, test_labels = next(iter(test_loader))\n",
    "    test_images = test_images[:num_samples].to(device)\n",
    "    \n",
    "    # Generate masks\n",
    "    sample_ids = test_labels[:num_samples].tolist()\n",
    "    cond_masks, target_masks = sparsity_controller.get_masks(num_samples, 3, sample_ids)\n",
    "    cond_mask = torch.stack(cond_masks).to(device)\n",
    "    target_mask = torch.stack(target_masks).to(device)\n",
    "    \n",
    "    # Create sparse input\n",
    "    sparse_input = test_images * cond_mask\n",
    "    \n",
    "    # Sample\n",
    "    samples = diffusion.sample(\n",
    "        batch_size=num_samples,\n",
    "        sparse_input=sparse_input,\n",
    "        mask=cond_mask,\n",
    "        clip=True\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    fig = visualize_samples(\n",
    "        test_images, sparse_input, cond_mask, samples, target_mask,\n",
    "        nrow=4, title=f\"Epoch {epoch+1}\"\n",
    "    )\n",
    "    plt.savefig(f'./results/samples_epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved samples for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "train_losses = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    avg_loss = train_epoch(epoch)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Sample images\n",
    "    if (epoch + 1) % SAVE_EVERY == 0 or epoch == 0:\n",
    "        sample_images(epoch, num_samples=16)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': diffusion.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'train_losses': train_losses\n",
    "        }\n",
    "        torch.save(checkpoint, f'./checkpoints/checkpoint_epoch_{epoch+1:03d}.pt')\n",
    "        print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('./results/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint if needed\n",
    "# checkpoint = torch.load('./checkpoints/checkpoint_epoch_050.pt')\n",
    "# diffusion.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "diffusion.eval()\n",
    "\n",
    "# Generate more samples\n",
    "num_samples = 64\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images = test_images[:num_samples].to(device)\n",
    "\n",
    "sample_ids = test_labels[:num_samples].tolist()\n",
    "cond_masks, target_masks = sparsity_controller.get_masks(num_samples, 3, sample_ids)\n",
    "cond_mask = torch.stack(cond_masks).to(device)\n",
    "target_mask = torch.stack(target_masks).to(device)\n",
    "sparse_input = test_images * cond_mask\n",
    "\n",
    "samples = diffusion.sample(\n",
    "    batch_size=num_samples,\n",
    "    sparse_input=sparse_input,\n",
    "    mask=cond_mask,\n",
    "    clip=True\n",
    ")\n",
    "\n",
    "# Save final samples\n",
    "fig = visualize_samples(\n",
    "    test_images, sparse_input, cond_mask, samples, target_mask,\n",
    "    nrow=8, title=\"Final Samples\"\n",
    ")\n",
    "plt.savefig('./results/final_samples.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Training and sampling completed!\")\n",
    "print(f\"Results saved in ./results/\")\n",
    "print(f\"Checkpoints saved in ./checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Sparsity-Aware Training**: Model receives 20% of pixels as conditioning\n",
    "2. **Target Loss Masking**: Loss computed on a different 20% of pixels\n",
    "3. **Full Reconstruction**: Model learns to reconstruct complete images (100%)\n",
    "4. **CIFAR-10 Application**: Trained on 32×32 RGB images\n",
    "\n",
    "### Key Results to Observe:\n",
    "- Loss should decrease over epochs\n",
    "- Generated samples should progressively improve\n",
    "- Model learns relationships between sparse observations and full images\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different sparsity levels (10%, 30%, etc.)\n",
    "- Try different sparsity patterns (blocks, grids, etc.)\n",
    "- Increase model capacity for better quality\n",
    "- Add DDIM sampling for faster inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
