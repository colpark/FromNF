# Sparsity-Aware Diffusion Model

A PyTorch implementation of a novel sparsity-aware diffusion model that learns to reconstruct full images from sparse observations.

## ğŸ¯ Key Innovation

The model demonstrates a remarkable capability:
- **Input**: 20% of image pixels (sparse conditioning)
- **Training Target**: Different 20% of pixels (loss computation)
- **Output**: Full 100% image reconstruction

This approach enables the model to learn complete image distributions from partial observations.

## ğŸ—ï¸ Architecture

### Sparsity-Aware Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Original Image (100%)                      â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Cond   â”‚    â”‚ Target  â”‚   â”‚ Unseen  â”‚
    â”‚  Mask   â”‚    â”‚  Mask   â”‚   â”‚   60%   â”‚
    â”‚  (20%)  â”‚    â”‚  (20%)  â”‚   â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚
         â”‚              â”‚
         â–¼              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  U-Net with  â”‚    â”‚
    â”‚  Diffusion   â”‚â—„â”€â”€â”€â”˜ (loss computed here)
    â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Reconstructedâ”‚
    â”‚ Full Image   â”‚
    â”‚   (100%)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Components

1. **SparsityController**: Manages mask generation
   - `random_epoch`: Consistent masks per sample within epoch
   - Separate conditioning and target masks
   - Configurable sparsity levels

2. **U-Net Architecture**:
   - Input channels: `3 Ã— C` (noised_image + sparse_input + mask)
   - Multi-resolution processing with attention
   - Time-conditioned residual blocks

3. **Gaussian Diffusion**:
   - 1000-step diffusion process
   - Weighted loss: 1.0 on target pixels, 0.05 on conditioning pixels
   - DDPM sampling with clipping

## ğŸ“Š CIFAR-10 Results

Trained on CIFAR-10 dataset (32Ã—32 RGB images):
- **Dataset**: 50,000 training images, 10,000 test images
- **Sparsity**: 20% conditioning + 20% target = 40% total supervision
- **Model**: ~15M parameters

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Training

Open and run the Jupyter notebook:

```bash
jupyter notebook sparsity_diffusion_cifar10.ipynb
```

The notebook is self-contained and includes:
- âœ… All model architectures
- âœ… Training loop with progress bars
- âœ… Visualization utilities
- âœ… Checkpoint saving/loading
- âœ… Sample generation

### Configuration

Key hyperparameters in the notebook:

```python
IMAGE_SIZE = 32          # CIFAR-10 image size
BATCH_SIZE = 128         # Batch size
LEARNING_RATE = 2e-4     # Adam learning rate
EPOCHS = 50              # Training epochs
SPARSITY = 0.2           # 20% sparsity level
```

## ğŸ“ Project Structure

```
FromNF/
â”œâ”€â”€ sparsity_diffusion_cifar10.ipynb  # Main training notebook
â”œâ”€â”€ ref/                               # Reference implementations
â”‚   â”œâ”€â”€ diffusion (8) (1).py          # Original diffusion code
â”‚   â”œâ”€â”€ model_original (7) (1).py     # Original U-Net model
â”‚   â””â”€â”€ trainer (6) (1).py            # Original trainer
â”œâ”€â”€ results/                           # Generated samples (created during training)
â”œâ”€â”€ checkpoints/                       # Model checkpoints (created during training)
â”œâ”€â”€ data/                              # CIFAR-10 dataset (auto-downloaded)
â”œâ”€â”€ README.md                          # This file
â””â”€â”€ requirements.txt                   # Python dependencies
```

## ğŸ”¬ How It Works

### Training Process

1. **Mask Generation**: For each sample, generate two non-overlapping 20% masks
   ```python
   cond_mask: Binary mask for conditioning pixels
   target_mask: Binary mask for loss computation (disjoint from cond_mask)
   ```

2. **Forward Pass**:
   ```python
   # Concatenate inputs
   model_input = [noised_image, sparse_input, cond_mask]

   # Predict noise
   predicted_noise = unet(model_input, timestep)

   # Compute weighted loss
   loss = weighted_mse(predicted_noise, true_noise, target_mask)
   ```

3. **Sampling**: During inference, use sparse conditioning to guide reconstruction
   ```python
   # Start from noise
   x_T ~ N(0, I)

   # Iteratively denoise with conditioning
   for t in [T, T-1, ..., 1]:
       x_{t-1} = denoise(x_t, sparse_input, mask, t)

   # Return final sample
   return x_0
   ```

### Why This Works

1. **Statistical Coverage**: Across batches, every pixel appears in both conditioning and target masks
2. **Contextual Learning**: Model learns spatial correlations from observed to unobserved pixels
3. **Iterative Refinement**: 1000-step reverse diffusion enables progressive reconstruction
4. **Implicit Regularization**: Small loss weight on conditioning pixels prevents overfitting

## ğŸ“ˆ Expected Results

### Training Progress

- **Early epochs (1-10)**: Blurry reconstructions, high loss
- **Mid-training (10-30)**: Recognizable structures, decreasing loss
- **Late training (30-50)**: Sharp details, stable loss

### Visualization Outputs

The notebook generates:
- Training loss curves
- Sample reconstructions every 5 epochs
- Visualization of masks and reconstructions
- Final high-quality samples

## ğŸ› ï¸ Customization

### Different Sparsity Levels

```python
sparsity_controller = SparsityController(
    image_size=32,
    sparsity=0.1,  # Try 10%, 30%, 40%
    ...
)
```

### Different Patterns

Extend `SparsityController` to support:
- Block-based sparsity
- Grid patterns
- Random walks
- Learned masks

### Model Scaling

```python
unet = Unet(
    dim=128,  # Increase from 64 for larger model
    dim_multiply=(1, 2, 4, 8, 16),  # Add more layers
    ...
)
```

## ğŸ“š Citation

If you use this code, please cite:

```bibtex
@misc{sparsity-aware-diffusion,
  title={Sparsity-Aware Diffusion Model},
  author={FromNF Team},
  year={2024},
  url={https://github.com/colpark/FromNF}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- [ ] DDIM sampler for faster inference
- [ ] Multi-scale training
- [ ] Conditional generation by class
- [ ] FID score evaluation
- [ ] Wandb integration

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

Based on:
- **DDPM**: Denoising Diffusion Probabilistic Models (Ho et al., 2020)
- **DDIM**: Denoising Diffusion Implicit Models (Song et al., 2021)
- Original reference implementation in `ref/` directory

---

**Status**: ğŸš€ Ready for experimentation

**Last Updated**: October 2024
